{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "from time import time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "torch.set_grad_enabled(False)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f225c552b80>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Neuroevolution Setup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class Agent(nn.Module):\n",
    "    '''The brain of the agent'''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(nn.Linear(4, 32),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(32, 2))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.fc(inputs)\n",
    "        return F.softmax(x, dim=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def initialize_population(pop_size=2):\n",
    "    '''Randomly initialize a bunch of agents'''\n",
    "    population = [Agent() for _ in range(pop_size)]\n",
    "    \n",
    "    return population"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def evaluate_agent(agent, episodes=15, max_episode_length=250):\n",
    "    '''\n",
    "    한 agent를 한번만 돌리는게 아니라 'episoides'수만큼 돌린 후 나온 reward들의 평균을 그 agent의 reward로 봄\n",
    "    예를 들어 한 세대의 agent를 3개(agent1,agent2,agent3)로 설정하고 episodes를 5로 설정하면\n",
    "    agent1-1, agent1-2, agent1-3, agent1-4, agent1-5의 reward들의 평균이 agent1의 최종 reward가 되는 것\n",
    "\n",
    "    max_episode_length는 얻을수있는 최대 reward를 의미\n",
    "    매 스텝(카트의 좌우움직임 한번)마다 reward가 +1이 됨\n",
    "    즉 설정한 max_episode_length만큼 스텝하는동안 막대를 안떨어뜨렸으면 그 episode의 reward는 max_episode_length와 같음 '''\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    agent.eval()\n",
    "    \n",
    "    total_rewards = []\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        observation = env.reset()\n",
    "        # Modify the maximum steps that can be taken in a single episode\n",
    "        env._max_episode_steps = max_episode_length\n",
    "        \n",
    "        episodic_reward = 0\n",
    "        # Start episode\n",
    "        for step in range(max_episode_length):\n",
    "            #env.render()    #렌더링 없애고 싶으면 이부분 주석처리\n",
    "            input_obs = torch.Tensor(observation).unsqueeze(0)\n",
    "            observation, reward, done, info = env.step(agent(input_obs).argmax(dim=1).item())\n",
    "            \n",
    "            episodic_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        total_rewards.append(episodic_reward)\n",
    "        \n",
    "                \n",
    "    return np.array(total_rewards).mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def evaluate_population(population, episodes=15, max_episode_length=250):\n",
    "    '''Evaluate the population'''\n",
    "    pop_fitness = []\n",
    "    \n",
    "    for agent in population:\n",
    "        pop_fitness.append(evaluate_agent(agent, episodes, max_episode_length))\n",
    "        \n",
    "    return pop_fitness"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "'''\n",
    "처음에 topology를 고정했기 때문에 crossover는 하지 않고 바로 mutation\n",
    "\n",
    "mutation방법: 부모 agnet의 parameter에 정규분포에서 뽑은 랜덤한 수 x mutation_power를 더해줌\n",
    "'''\n",
    "\n",
    "def mutate(parent_agent, mutation_power=0.02):\n",
    "    child_agent = copy.deepcopy(parent_agent)\n",
    "    \n",
    "    for param in child_agent.parameters():\n",
    "        param.data = param.data + (torch.randn(param.shape) * mutation_power)\n",
    "        \n",
    "    return child_agent"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def repopulate(top_agents, pop_size, mutation_power):\n",
    "    '''Repopulate the population from the top agents by mutation'''\n",
    "    new_population = []\n",
    "    \n",
    "    n = 0\n",
    "    while(n < pop_size):\n",
    "        for parent in top_agents:\n",
    "            child = mutate(parent, mutation_power)\n",
    "            new_population.append(child)\n",
    "            n += 1\n",
    "            \n",
    "    return new_population[:pop_size - 1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "TRAINED_AGENT = {}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "'''\n",
    "정해진 타임만큼 학습할건지 정해진 세대만큼 학습할건지 정할수 있음 (아래 주석 참고)\n",
    "'''\n",
    "\n",
    "def evolve(generations=10, max_time=80, \n",
    "           pop_size=100, \n",
    "           topK=20, \n",
    "           episodes=15, \n",
    "           max_episode_length=250, \n",
    "           mutation_power=0.02):\n",
    "    '''\n",
    "    topK: 자손세대를 만들 부모세대 agent수\n",
    "    예를 들어 pop_size=100, topK=20이면 부모세대의 agent 100개 중 reward가 가장 높은 20개의 agent들로 자식세대 agent100개를 만듦\n",
    "    '''\n",
    "    \n",
    "    global TRAINED_AGENT\n",
    "    \n",
    "    population = initialize_population(pop_size)\n",
    "    global_best = {}\n",
    "    \n",
    "    t1 = time()\n",
    "    #g = 0 # 정해진 타임만큼 돌리고 싶을때 \n",
    "    for g in range(generations):      # 정해진 세대만큼 돌릴 때\n",
    "    #while ((time() - t1) <= max_time): # 정해진 타임만큼 돌리고 싶을때 \n",
    "        \n",
    "        # Evaluate the population\n",
    "        pop_fitness = evaluate_population(population, episodes, max_episode_length)\n",
    "        mean_pop_reward = np.array(pop_fitness).mean()\n",
    "        \n",
    "        # Rank the agents in descending order\n",
    "        topK_idx = np.argsort(pop_fitness)[::-1][:topK]\n",
    "        topK_agents = [population[i] for i in topK_idx]\n",
    "        \n",
    "        # Get Best Agent\n",
    "        best_agent = population[topK_idx[0]]\n",
    "        best_reward = pop_fitness[topK_idx[0]]\n",
    "        \n",
    "        # Check with global best\n",
    "        if g == 0:\n",
    "            global_best['reward'] = best_reward\n",
    "            global_best['agent'] = best_agent\n",
    "        else:\n",
    "            if best_reward >= global_best['reward']:\n",
    "                global_best['reward'] = best_reward\n",
    "                global_best['agent'] = best_agent\n",
    "                \n",
    "        print('Generation', g)\n",
    "        print('Mean Reward of Population', mean_pop_reward)\n",
    "        print('Best Agent Reward (mean)', best_reward)\n",
    "        print('Global Best Reward (mean)', global_best['reward'], '\\n')\n",
    "        \n",
    "        # Mutate and Repopulate\n",
    "        new_population = repopulate(topK_agents, pop_size, mutation_power)\n",
    "        # take the best agent of generation forward without cloning as well\n",
    "        new_population.append(best_agent)\n",
    "        \n",
    "        population = new_population\n",
    "        \n",
    "        TRAINED_AGENT = global_best\n",
    "        \n",
    "        #g += 1 # 정해진 타임만큼 돌리고 싶을때 "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "evolve(generations=20,\n",
    "       pop_size=20, \n",
    "       topK=10, \n",
    "       episodes=15, \n",
    "       max_episode_length=200, \n",
    "       mutation_power=0.02)\n",
    "#총 렌더링 수: generations x pop_size x episodes"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Generation 0\n",
      "Mean Reward of Population 14.703333333333337\n",
      "Best Agent Reward (mean) 65.4\n",
      "Global Best Reward (mean) 65.4 \n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test the Trained Agent"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def play_agent(agent, episodes=5, max_episode_length=200, render=False):\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    \n",
    "    agent.eval()\n",
    "    \n",
    "    total_rewards = []\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        observation = env.reset()\n",
    "        env._max_episode_steps = max_episode_length\n",
    "        \n",
    "        episodic_reward = 0\n",
    "        \n",
    "        for step in range(max_episode_length):\n",
    "            if render:\n",
    "                env.render()\n",
    "            \n",
    "            input_obs = torch.Tensor(observation).unsqueeze(0)\n",
    "            observation, reward, done, info = env.step(agent(input_obs).argmax(dim=1).item())\n",
    "            \n",
    "            episodic_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        \n",
    "        total_rewards.append(episodic_reward)\n",
    "    \n",
    "    env.close()\n",
    "    print('Mean Rewards across all episodes', np.array(total_rewards).mean())\n",
    "    print('Best Reward in any single episode', max(total_rewards))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "'''위에 evolve함수로 학습한 것들 중 가장 마지막 세대의 cartpole학습을 볼 수 있음'''\n",
    "play_agent(TRAINED_AGENT['agent'], episodes=5, max_episode_length=200, render=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean Rewards across all episodes 59.6\n",
      "Best Reward in any single episode 70.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "torch.save(TRAINED_AGENT['agent'].state_dict(), 'model-200.pth')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}