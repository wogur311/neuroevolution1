{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "from time import time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "torch.set_grad_enabled(False)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f813030f5b0>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Neuroevolution Setup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class Agent(nn.Module):\n",
    "    '''The brain of the agent'''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(nn.Linear(4, 32),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(32, 2))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.fc(inputs)\n",
    "        return F.softmax(x, dim=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def initialize_population(pop_size=2):\n",
    "    '''Randomly initialize a bunch of agents'''\n",
    "    population = [Agent() for _ in range(pop_size)]\n",
    "    \n",
    "    return population"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def evaluate_agent(agent, episodes=15, max_episode_length=250):\n",
    "    '''\n",
    "    한 agent를 한번만 돌리는게 아니라 'episoides'수만큼 돌린 후 나온 reward들의 평균을 그 agent의 reward로 봄\n",
    "    예를 들어 한 세대의 agent를 3개(agent1,agent2,agent3)로 설정하고 episodes를 5로 설정하면\n",
    "    agent1-1, agent1-2, agent1-3, agent1-4, agent1-5의 reward들의 평균이 agent1의 최종 reward가 되는 것\n",
    "\n",
    "    max_episode_length는 얻을수있는 최대 reward를 의미\n",
    "    매 스텝(카트의 좌우움직임 한번)마다 reward가 +1이 됨\n",
    "    즉 설정한 max_episode_length만큼 스텝하는동안 막대를 안떨어뜨렸으면 그 episode의 reward는 max_episode_length와 같음 '''\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    agent.eval()\n",
    "    \n",
    "    total_rewards = []\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        observation = env.reset()\n",
    "        # Modify the maximum steps that can be taken in a single episode\n",
    "        env._max_episode_steps = max_episode_length\n",
    "        \n",
    "        episodic_reward = 0\n",
    "        # Start episode\n",
    "        for step in range(max_episode_length):\n",
    "            input_obs = torch.Tensor(observation).unsqueeze(0)\n",
    "            observation, reward, done, info = env.step(agent(input_obs).argmax(dim=1).item())\n",
    "            \n",
    "            episodic_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        total_rewards.append(episodic_reward)\n",
    "                \n",
    "    return np.array(total_rewards).mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def evaluate_population(population, episodes=15, max_episode_length=250):\n",
    "    '''Evaluate the population'''\n",
    "    pop_fitness = []\n",
    "    \n",
    "    for agent in population:\n",
    "        pop_fitness.append(evaluate_agent(agent, episodes, max_episode_length))\n",
    "        \n",
    "    return pop_fitness"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "'''\n",
    "처음에 topology를 고정했기 때문에 crossover는 하지 않고 바로 mutation\n",
    "\n",
    "mutation방법: 부모 agnet의 parameter에 정규분포에서 뽑은 랜덤한 수 x mutation_power를 더해줌\n",
    "'''\n",
    "\n",
    "def mutate(parent_agent, mutation_power=0.02):\n",
    "    child_agent = copy.deepcopy(parent_agent)\n",
    "    \n",
    "    for param in child_agent.parameters():\n",
    "        param.data = param.data + (torch.randn(param.shape) * mutation_power)\n",
    "        \n",
    "    return child_agent"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def repopulate(top_agents, pop_size, mutation_power):\n",
    "    '''Repopulate the population from the top agents by mutation'''\n",
    "    new_population = []\n",
    "    \n",
    "    n = 0\n",
    "    while(n < pop_size):\n",
    "        for parent in top_agents:\n",
    "            child = mutate(parent, mutation_power)\n",
    "            new_population.append(child)\n",
    "            n += 1\n",
    "            \n",
    "    return new_population[:pop_size - 1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "TRAINED_AGENT = {}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def evolve(generations=10, max_time=60, \n",
    "           pop_size=100, \n",
    "           topK=20, \n",
    "           episodes=15, \n",
    "           max_episode_length=250, \n",
    "           mutation_power=0.02):\n",
    "    '''\n",
    "    topK: 자손세대를 만들 부모세대 agent수\n",
    "    예를 들어 pop_size=100, topK=20이면 부모세대의 agent 100개 중 reward가 가장 높은 20개의 agent들로 자식세대 agent100개를 만듦\n",
    "    '''\n",
    "    \n",
    "    global TRAINED_AGENT\n",
    "    \n",
    "    population = initialize_population(pop_size)\n",
    "    global_best = {}\n",
    "    \n",
    "    t1 = time()\n",
    "#     g = 0 # uncomment when using max_time for training instead of generations\n",
    "    for g in range(generations):\n",
    "#     while ((time() - t1) <= max_time): # uncomment when using max_time for training instead of generations\n",
    "        \n",
    "        # Evaluate the population\n",
    "        pop_fitness = evaluate_population(population, episodes, max_episode_length)\n",
    "        mean_pop_reward = np.array(pop_fitness).mean()\n",
    "        \n",
    "        # Rank the agents in descending order\n",
    "        topK_idx = np.argsort(pop_fitness)[::-1][:topK]\n",
    "        topK_agents = [population[i] for i in topK_idx]\n",
    "        \n",
    "        # Get Best Agent\n",
    "        best_agent = population[topK_idx[0]]\n",
    "        best_reward = pop_fitness[topK_idx[0]]\n",
    "        \n",
    "        # Check with global best\n",
    "        if g == 0:\n",
    "            global_best['reward'] = best_reward\n",
    "            global_best['agent'] = best_agent\n",
    "        else:\n",
    "            if best_reward >= global_best['reward']:\n",
    "                global_best['reward'] = best_reward\n",
    "                global_best['agent'] = best_agent\n",
    "                \n",
    "        print('Generation', g)\n",
    "        print('Mean Reward of Population', mean_pop_reward)\n",
    "        print('Best Agent Reward (mean)', best_reward)\n",
    "        print('Global Best Reward (mean)', global_best['reward'], '\\n')\n",
    "        \n",
    "        # Mutate and Repopulate\n",
    "        new_population = repopulate(topK_agents, pop_size, mutation_power)\n",
    "        # take the best agent of generation forward without cloning as well\n",
    "        new_population.append(best_agent)\n",
    "        \n",
    "        population = new_population\n",
    "        \n",
    "        TRAINED_AGENT = global_best\n",
    "        \n",
    "#         g += 1 # uncomment when using max_time for training instead of generations"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "evolve(generations=20,\n",
    "       pop_size=20, \n",
    "       topK=10, \n",
    "       episodes=15, \n",
    "       max_episode_length=200, \n",
    "       mutation_power=0.02)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/gym/envs/registration.py:505: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1` with the environment ID `CartPole-v1`.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/pygame/surfarray.py:53: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if hasattr(numpy, type_name):\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Generation 0\n",
      "Mean Reward of Population 12.003333333333334\n",
      "Best Agent Reward (mean) 45.333333333333336\n",
      "Global Best Reward (mean) 45.333333333333336 \n",
      "\n",
      "Generation 1\n",
      "Mean Reward of Population 15.453333333333333\n",
      "Best Agent Reward (mean) 46.4\n",
      "Global Best Reward (mean) 46.4 \n",
      "\n",
      "Generation 2\n",
      "Mean Reward of Population 25.369999999999997\n",
      "Best Agent Reward (mean) 49.8\n",
      "Global Best Reward (mean) 49.8 \n",
      "\n",
      "Generation 3\n",
      "Mean Reward of Population 38.39\n",
      "Best Agent Reward (mean) 81.86666666666666\n",
      "Global Best Reward (mean) 81.86666666666666 \n",
      "\n",
      "Generation 4\n",
      "Mean Reward of Population 52.416666666666664\n",
      "Best Agent Reward (mean) 112.66666666666667\n",
      "Global Best Reward (mean) 112.66666666666667 \n",
      "\n",
      "Generation 5\n",
      "Mean Reward of Population 55.126666666666665\n",
      "Best Agent Reward (mean) 99.13333333333334\n",
      "Global Best Reward (mean) 112.66666666666667 \n",
      "\n",
      "Generation 6\n",
      "Mean Reward of Population 61.006666666666675\n",
      "Best Agent Reward (mean) 104.33333333333333\n",
      "Global Best Reward (mean) 112.66666666666667 \n",
      "\n",
      "Generation 7\n",
      "Mean Reward of Population 66.72333333333333\n",
      "Best Agent Reward (mean) 116.46666666666667\n",
      "Global Best Reward (mean) 116.46666666666667 \n",
      "\n",
      "Generation 8\n",
      "Mean Reward of Population 70.07000000000001\n",
      "Best Agent Reward (mean) 123.66666666666667\n",
      "Global Best Reward (mean) 123.66666666666667 \n",
      "\n",
      "Generation 9\n",
      "Mean Reward of Population 78.96666666666667\n",
      "Best Agent Reward (mean) 128.13333333333333\n",
      "Global Best Reward (mean) 128.13333333333333 \n",
      "\n",
      "Generation 10\n",
      "Mean Reward of Population 78.97333333333333\n",
      "Best Agent Reward (mean) 141.66666666666666\n",
      "Global Best Reward (mean) 141.66666666666666 \n",
      "\n",
      "Generation 11\n",
      "Mean Reward of Population 81.23\n",
      "Best Agent Reward (mean) 119.93333333333334\n",
      "Global Best Reward (mean) 141.66666666666666 \n",
      "\n",
      "Generation 12\n",
      "Mean Reward of Population 68.91666666666667\n",
      "Best Agent Reward (mean) 129.0\n",
      "Global Best Reward (mean) 141.66666666666666 \n",
      "\n",
      "Generation 13\n",
      "Mean Reward of Population 64.93000000000002\n",
      "Best Agent Reward (mean) 119.13333333333334\n",
      "Global Best Reward (mean) 141.66666666666666 \n",
      "\n",
      "Generation 14\n",
      "Mean Reward of Population 82.39333333333333\n",
      "Best Agent Reward (mean) 161.6\n",
      "Global Best Reward (mean) 161.6 \n",
      "\n",
      "Generation 15\n",
      "Mean Reward of Population 81.54333333333335\n",
      "Best Agent Reward (mean) 124.26666666666667\n",
      "Global Best Reward (mean) 161.6 \n",
      "\n",
      "Generation 16\n",
      "Mean Reward of Population 71.24666666666666\n",
      "Best Agent Reward (mean) 137.13333333333333\n",
      "Global Best Reward (mean) 161.6 \n",
      "\n",
      "Generation 17\n",
      "Mean Reward of Population 70.37666666666667\n",
      "Best Agent Reward (mean) 118.13333333333334\n",
      "Global Best Reward (mean) 161.6 \n",
      "\n",
      "Generation 18\n",
      "Mean Reward of Population 93.23000000000002\n",
      "Best Agent Reward (mean) 171.13333333333333\n",
      "Global Best Reward (mean) 171.13333333333333 \n",
      "\n",
      "Generation 19\n",
      "Mean Reward of Population 83.15\n",
      "Best Agent Reward (mean) 128.73333333333332\n",
      "Global Best Reward (mean) 171.13333333333333 \n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test the Trained Agent"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def play_agent(agent, episodes=5, max_episode_length=200, render=False):\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    \n",
    "    agent.eval()\n",
    "    \n",
    "    total_rewards = []\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        observation = env.reset()\n",
    "        env._max_episode_steps = max_episode_length\n",
    "        \n",
    "        episodic_reward = 0\n",
    "        \n",
    "        for step in range(max_episode_length):\n",
    "            if render:\n",
    "                env.render()\n",
    "            \n",
    "            input_obs = torch.Tensor(observation).unsqueeze(0)\n",
    "            observation, reward, done, info = env.step(agent(input_obs).argmax(dim=1).item())\n",
    "            \n",
    "            episodic_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        \n",
    "        total_rewards.append(episodic_reward)\n",
    "    \n",
    "    env.close()\n",
    "    print('Mean Rewards across all episodes', np.array(total_rewards).mean())\n",
    "    print('Best Reward in any single episode', max(total_rewards))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "play_agent(TRAINED_AGENT['agent'], episodes=100, max_episode_length=200, render=True)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'play_agent' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplay_agent\u001b[49m(TRAINED_AGENT[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124magent\u001b[39m\u001b[38;5;124m'\u001b[39m], episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, max_episode_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, render\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'play_agent' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "torch.save(TRAINED_AGENT['agent'].state_dict(), 'model-200.pth')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}